{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To clean `JSON` weather and taxi avail files called from NEA and LTA sites <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gcsfs\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import joblib\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely import geometry\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from src import jsonParser\n",
    "from src import assignment\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing weather and taxi tabular files \n",
    "Merge the data together into 1 dataframe<br>\n",
    "This is only for ONE timestamp. Timestamps should be cleaned min is either 00 or 30; sec is 00<br>\n",
    "Eg 2019-01-01T12:30:00 <br>\n",
    "\n",
    "Weather files: <br>\n",
    "1. rainfall w stations value     ---   timestamp | station_id | value\n",
    "2. rainfall w stations latlon    ---  timestamp | station_id | latitude | longitude\n",
    "3. humidity w stations value     ---   timestamp | station_id | value\n",
    "4. humidity w stations latlon    ---   timestamp | station_id | latitude | longitude\n",
    "5. temperature w stations value  ---  timestamp | station_id | value\n",
    "6. temperature w stations latlon --- timestamp | station_id | latitude | longitude\n",
    "<br>\n",
    "\n",
    "Taxi files: <br>\n",
    "7. taxi avail json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'ml-eng-cs611-group-project'\n",
    "nea_bucket = 'ml-eng-cs611-group-project-nea'\n",
    "taxi_bucket = 'ml-eng-cs611-group-project-taxis'\n",
    "dataset_id='taxi_dataset'\n",
    "measure = 'rainfall'\n",
    "measures = ['rainfall','air-temperature','relative-humidity']\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=project)\n",
    "nea_filenames = fs.glob('/'.join([nea_bucket,measure,\"*\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read grid file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_file = './updated codes/filter_grids_2/filter_grids_2.shp' ## To change directory\n",
    "\n",
    "grids = gpd.read_file(grids_file)\n",
    "grids['centroid'] = grids['geometry'].apply(lambda x: x.centroid) # get grids' centroid\n",
    "\n",
    "# convert to dataframe\n",
    "grids_df = pd.DataFrame(grids)\n",
    "grids_df['centroid'] = grids_df['centroid'].astype(str)\n",
    "grids_df['latlon'] = grids_df['centroid'].apply(lambda x: (float(x.split(' ')[1][1:]), float(x.split(' ')[2][:-1])))\n",
    "\n",
    "# Get unique grid_num\n",
    "grid_nums = list(grids_df['grid_num'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weather datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nea_data = {}\n",
    "\n",
    "for measure in measures:\n",
    "    filenames = fs.glob('/'.join([nea_bucket,measure,\"*\"]))\n",
    "    file = filenames[0]\n",
    "    parser = jsonParser.jsonParser(fs)\n",
    "    \n",
    "    items = parser.get_items(file,measure)    \n",
    "    metadata = parser.get_metadata(file,measure)\n",
    "    \n",
    "    nea_data[measure]={'items':items,'metadata':metadata}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing weather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get weather value for each grid at each timestamp. Returns a dataframe\n",
    "def get_value(weather, df, df_stn, ts, grid_nums, grids_df):\n",
    "    '''\n",
    "    Arguments\n",
    "    weather: temp/ rain/ humid\n",
    "    df: dataframe that contains weather value\n",
    "    df_stn: dataframe that contains stn latlon\n",
    "    ts: timestamp\n",
    "    grid_nums: list of unique grid numbers\n",
    "    grids_df: dataframe that contains grid numbers and their centroid latlon\n",
    "    '''\n",
    "    value_list = []\n",
    "    df_fil = df[df['timestamp'] == ts].reset_index()\n",
    "    df_stn_fil = df_stn[df_stn['timestamp'] == ts].reset_index()\n",
    "    \n",
    "    for i in range(len(grid_nums)): # for each grid_num\n",
    "        a = grids_df.iloc[i]['latlon'] # latlon of row i grid_num\n",
    "        stn_id = 0\n",
    "        shortest = 1000000\n",
    "        for j in range(len(df_stn_fil)): # for each station\n",
    "            b = df_stn_fil.iloc[j]['latlon']\n",
    "            interim = distance.euclidean(a, b) # get euclidean\n",
    "            if interim < shortest:\n",
    "                stn_id = df_stn_fil.iloc[j]['station_id']\n",
    "                shortest = interim\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # after getting the nearest stn_id\n",
    "        value = df_fil[df_fil['station_id'] == stn_id].reset_index()['value'][0] # get value\n",
    "        value_list.append(value) # append value\n",
    "    \n",
    "    df_interim = pd.DataFrame({'grid_num': grid_nums, 'timestamp': [ts for x in range(len(grid_nums))], \n",
    "                               f'{weather}': value_list})\n",
    "    \n",
    "    return df_interim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df_temp_stn.iloc[0]['timestamp'] ## To change if there are other ways to get the timestamp\n",
    "\n",
    "temp_clean = get_value('temp', nea_data['air-temperature']['items'], nea_data['air-temperature']['metadata'], ts, grid_nums, grids_df)\n",
    "rain_clean = get_value('rain', nea_data['rainfall']['items'], nea_data['rainfall']['metadata'], ts, grid_nums, grids_df)\n",
    "humid_clean = get_value('humid', nea_data['relative-humidity']['items'], nea_data['relative-humidity']['metadata'], ts, grid_nums, grids_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load taxi json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_file = 'taxi_avail.joblib' # change directory and extension of file accordingly\n",
    "taxi = joblib.load(taxi_file) # change reading method according to the file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing taxi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of taxi's coord\n",
    "one_list = taxi['features'][0]['geometry']['coordinates']\n",
    "# Convert list to corr grid num of the coord\n",
    "test = [math.ceil((i[0]-103.6)/0.020454545454545583) + (13 - math.ceil((i[1] -1.208)/0.020538461538461547))*22 for i in one_list]\n",
    "\n",
    "# getting dictionary of items\n",
    "c = Counter(test)\n",
    "\n",
    "# Getting taxi_count for relevant grid_num\n",
    "df_taxicount = pd.DataFrame({'grid_num': [float(x) for x in list(c.keys())], \n",
    "                             'taxi_count': [x[1] for x in list(c.items())]})\n",
    "\n",
    "# Get full list of grid_num as a dataframe:  grid_num | timestamp\n",
    "all_grids = grids[['grid_num']]\n",
    "all_grids['timestamp'] = ts\n",
    "\n",
    "\n",
    "# Merge all_grids and df_taxicount\n",
    "taxi_clean = pd.merge(all_grids, df_taxicount, how='left')\n",
    "taxi_clean['taxi_count'] = taxi_clean['taxi_count'].fillna(0) #fill missing taxi_count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging all the cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge all together\n",
    "merge_df = pd.merge(humid_clean, rain_clean)\n",
    "merge_df = pd.merge(merge_df, temp_clean)\n",
    "merge_df = pd.merge(merge_df, taxi_clean)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['timestamp'] = merge_df['timestamp'].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S'))\n",
    "merge_df['hour'] = merge_df['timestamp'].apply(lambda x: x.hour)\n",
    "merge_df['month'] = merge_df['timestamp'].apply(lambda x: x.month)\n",
    "merge_df['day'] = merge_df['timestamp'].apply(lambda x: x.weekday())\n",
    "merge_df['minute'] = merge_df['timestamp'].apply(lambda x: x.minute)\n",
    "\n",
    "merge_df['time_30'] = merge_df['timestamp'].apply(lambda x: x + timedelta(hours=0.5))\n",
    "merge_df['time_60'] = merge_df['timestamp'].apply(lambda x: x + timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get y_30 and y_60 targets\n",
    "# for each timestamp, get 30min later\n",
    "merge_df['y_30'] = np.nan\n",
    "merge_df['y_60'] = np.nan\n",
    "\n",
    "for i in tqdm(range(len(merge_df))): # for each row\n",
    "    ts = merge_df.iloc[i]['time_30']\n",
    "    gridnum = merge_df.iloc[i]['grid_num']\n",
    "    \n",
    "    merge_df.iloc[i, merge_df.columns.get_loc('y_30')] = merge_df[(merge_df['grid_num'] == gridnum) & \n",
    "                                                                       (merge_df['timestamp'] == ts)].reset_index()['taxi_count'][0]\n",
    "    \n",
    "\n",
    "for i in tqdm(range(len(merge_df))): # for each row\n",
    "    ts = merge_df.iloc[i]['time_60']\n",
    "    gridnum = merge_df.iloc[i]['grid_num']\n",
    "    \n",
    "    merge_df.iloc[i, merge_df.columns.get_loc('y_60')] = merge_df[(merge_df['grid_num'] == gridnum) & \n",
    "                                                                       (merge_df['timestamp'] == ts)].reset_index()['taxi_count'][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge_df is the final dataset, ready to be used for EDA/ training etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m92",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m92"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
