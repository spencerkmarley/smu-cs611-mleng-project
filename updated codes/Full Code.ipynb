{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Code\n",
    "To clean json weather and taxi avail files called from NEA and LTA sites <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import joblib\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely import geometry\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gridding Singapore\n",
    "Output shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the 4 corners of SG\n",
    "TL, TR, BL, BR = np.array((103.6, 1.475)), np.array((104.05, 1.475)), np.array((103.6, 1.208)), np.array((104.05, 1.208))\n",
    "length_km = np.linalg.norm(TR - TL)*100\n",
    "print('length', length_km)\n",
    "width_km = np.linalg.norm(TL - BL)*100\n",
    "print('width', width_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to plot grids\n",
    "def grid(size): # in metres\n",
    "    '''\n",
    "    To divide SG area into grids\n",
    "    \n",
    "    argument\n",
    "    size: length of each grid in metres\n",
    "    \n",
    "    Req files\n",
    "    1) SG shape file\n",
    "    '''\n",
    "    # Read SG map\n",
    "    gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "    whole_sg = gpd.read_file('../singapore-police-force-npc-boundary/spf-boundaries.kml', driver='KML')\n",
    "    \n",
    "    # Divide length and widths\n",
    "    vertical = round(45000/size)\n",
    "    horizontal = round(26700/size)\n",
    "    \n",
    "    length_blocks = np.linspace(103.6, 104.05, num=vertical+1) #x\n",
    "    width_blocks = np.linspace(1.208, 1.475, num=horizontal+1) #y\n",
    "    \n",
    "    # Plotting\n",
    "    y = [1.475, 1.475, 1.208, 1.208]\n",
    "    x = [103.6, 104.05, 103.6, 104.05]\n",
    "    \n",
    "    whole_sg.plot(figsize=(15,15))\n",
    "    corner = sns.scatterplot(x=x,y=y, color = 'y')\n",
    "    \n",
    "    for i in length_blocks:\n",
    "        sns.lineplot(x=[i,i], y=[1.208,1.475], color = 'black')\n",
    "\n",
    "    for i in width_blocks:\n",
    "        sns.lineplot(x=[103.6, 104.05], y=[i,i], color = 'black')\n",
    "        \n",
    "#grid(2000)\n",
    "#plt.scatter(103.6267, 1.307992, c= 'r')\n",
    "\n",
    "## Function to know start end of grids\n",
    "def blocks_coord(size): # in metres\n",
    "    # Divide length and widths\n",
    "    vertical = round(45000/size)\n",
    "    horizontal = round(26700/size)\n",
    "    \n",
    "    length_blocks = np.linspace(103.6, 104.05, num=vertical+1) #x\n",
    "    width_blocks = np.linspace(1.475, 1.208, num=horizontal+1) #y\n",
    "    \n",
    "    return length_blocks, width_blocks\n",
    "\n",
    "length_blocks, rev_width_blocks = blocks_coord(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Dataframe\n",
    "grid_df = pd.DataFrame(\n",
    "    {'grid_num': [],\n",
    "    'Coordinates':[]})\n",
    "\n",
    "count = 1\n",
    "for i in range(len(rev_width_blocks)-1): # fix y first, \n",
    "    for j in range(len(length_blocks)-1): # then keep looping x (going to the right)\n",
    "        # Top L, Top R, Btm R, Btm L\n",
    "        p1 = geometry.Point(length_blocks[j],rev_width_blocks[i])\n",
    "        p2 = geometry.Point(length_blocks[j+1],rev_width_blocks[i])\n",
    "        p3 = geometry.Point(length_blocks[j+1],rev_width_blocks[i+1])\n",
    "        p4 = geometry.Point(length_blocks[j],rev_width_blocks[i+1])\n",
    "        \n",
    "        pointList = [p1,p2,p3,p4]\n",
    "        poly = geometry.Polygon([[p.x, p.y] for p in pointList])\n",
    "        \n",
    "        grid_df = grid_df.append(pd.DataFrame(\n",
    "                            {'grid_num': count,\n",
    "                            'Coordinates':[str(poly)]}), ignore_index=True)\n",
    "        count +=1\n",
    "\n",
    "        \n",
    "## Convert to geopandas dataframe:  grid_num|geometry\n",
    "grid_df['geometry'] = grid_df.Coordinates.apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(grid_df, geometry='geometry')\n",
    "gdf.drop('Coordinates', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To filter for grids that contains SG area\n",
    "gdf['intersect'] = False\n",
    "for squares in range(len(gdf)):\n",
    "    sq = gdf['geometry'][squares]\n",
    "    for plg in range(len(whole_sg)):\n",
    "        poly = whole_sg['geometry'][plg]\n",
    "        if poly.intersects(sq):\n",
    "            gdf.loc[squares, \"intersect\"] = True\n",
    "            break\n",
    "            \n",
    "filter_grids = all_grids[all_grids['intersect'] == True]\n",
    "\n",
    "## Export shape file\n",
    "filter_grids.to_file('filter_grids_2/filter_grids_2.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing weather and taxi tabular files \n",
    "Merge the data together into 1 dataframe<br>\n",
    "This is only for ONE timestamp. Timestamps should be cleaned min is either 00 or 30; sec is 00<br>\n",
    "Eg 2019-01-01T12:30:00 <br>\n",
    "\n",
    "Weather files: <br>\n",
    "1. rainfall w stations value     ---   timestamp | station_id | value\n",
    "2. rainfall w stations latlon    ---  timestamp | station_id | latitude | longitude\n",
    "3. humidity w stations value     ---   timestamp | station_id | value\n",
    "4. humidity w stations latlon    ---   timestamp | station_id | latitude | longitude\n",
    "5. temperature w stations value  ---  timestamp | station_id | value\n",
    "6. temperature w stations latlon --- timestamp | station_id | latitude | longitude\n",
    "<br>\n",
    "\n",
    "Taxi files: <br>\n",
    "7. taxi avail json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read grid file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_file = '../filter_grids_2/filter_grids_2.shp' ## To change directory\n",
    "\n",
    "grids = gpd.read_file(grids_file)\n",
    "grids['centroid'] = grids['geometry'].apply(lambda x: x.centroid) # get grids' centroid\n",
    "\n",
    "# convert to dataframe\n",
    "grids_df = pd.DataFrame(grids)\n",
    "grids_df['centroid'] = grids_df['centroid'].astype(str)\n",
    "grids_df['latlon'] = grids_df['centroid'].apply(lambda x: (float(x.split(' ')[1][1:]), float(x.split(' ')[2][:-1])))\n",
    "\n",
    "# Get unique grid_num\n",
    "grid_nums = list(grids_df['grid_num'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weather datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To change the 6 file directories! The files should all be of the same timestamp\n",
    "rain_value = 'rain_v.csv'\n",
    "rain_latlon = 'rain_l.csv'\n",
    "temp_value = 'temp_v.csv'\n",
    "temp_latlon = 'temp_l.csv'\n",
    "humid_value = 'humid_v.csv'\n",
    "humid_latlon = 'humid_l.csv'\n",
    "\n",
    "df_temp = pd.read_csv(temp_value)\n",
    "df_rain = pd.read_csv(rain_value)\n",
    "df_humid = pd.read_csv(humid_value)\n",
    "\n",
    "df_temp_stn = pd.read_csv(temp_latlon)\n",
    "df_rain_stn = pd.read_csv(rain_latlon)\n",
    "df_humid_stn = pd.read_csv(humid_latlon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing weather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get station latlon as a tuple\n",
    "df_temp_stn['latlon'] = df_temp_stn.apply(lambda x: (x.longitude, x.latitude), axis=1)\n",
    "df_rain_stn['latlon'] = df_rain_stn.apply(lambda x: (x.longitude, x.latitude), axis=1)\n",
    "df_humid_stn['latlon'] = df_humid_stn.apply(lambda x: (x.longitude, x.latitude), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get weather value for each grid at each timestamp. Returns a dataframe\n",
    "def get_value(weather, df, df_stn, ts, grid_nums, grids_df):\n",
    "    '''\n",
    "    Arguments\n",
    "    weather: temp/ rain/ humid\n",
    "    df: dataframe that contains weather value\n",
    "    df_stn: dataframe that contains stn latlon\n",
    "    ts: timestamp\n",
    "    grid_nums: list of unique grid numbers\n",
    "    grids_df: dataframe that contains grid numbers and their centroid latlon\n",
    "    '''\n",
    "    value_list = []\n",
    "    df_fil = df[df['timestamp'] == ts].reset_index()\n",
    "    df_stn_fil = df_stn[df_stn['timestamp'] == ts].reset_index()\n",
    "    \n",
    "    for i in range(len(grid_nums)): # for each grid_num\n",
    "        a = grids_df.iloc[i]['latlon'] # latlon of row i grid_num\n",
    "        stn_id = 0\n",
    "        shortest = 1000000\n",
    "        for j in range(len(df_stn_fil)): # for each station\n",
    "            b = df_stn_fil.iloc[j]['latlon']\n",
    "            interim = distance.euclidean(a, b) # get euclidean\n",
    "            if interim < shortest:\n",
    "                stn_id = df_stn_fil.iloc[j]['station_id']\n",
    "                shortest = interim\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # after getting the nearest stn_id\n",
    "        value = df_fil[df_fil['station_id'] == stn_id].reset_index()['value'][0] # get value\n",
    "        value_list.append(value) # append value\n",
    "    \n",
    "    df_interim = pd.DataFrame({'grid_num': grid_nums, 'timestamp': [ts for x in range(len(grid_nums))], \n",
    "                               f'{weather}': value_list})\n",
    "    \n",
    "    return df_interim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df_temp_stn.iloc[0]['timestamp'] ## To change if there are other ways to get the timestamp\n",
    "\n",
    "temp_clean = get_value('temp', df_temp, df_temp_stn, ts, grid_nums, grids_df)\n",
    "rain_clean = get_value('rain', df_rain, df_rain_stn, ts, grid_nums, grids_df)\n",
    "humid_clean = get_value('humid', df_humid, df_humid_stn, ts, grid_nums, grids_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load taxi json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_file = 'taxi_avail.joblib' # change directory and extension of file accordingly\n",
    "taxi = joblib.load(taxi_file) # change reading method according to the file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing taxi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of taxi's coord\n",
    "one_list = taxi['features'][0]['geometry']['coordinates']\n",
    "# Convert list to corr grid num of the coord\n",
    "test = [math.ceil((i[0]-103.6)/0.020454545454545583) + (13 - math.ceil((i[1] -1.208)/0.020538461538461547))*22 for i in one_list]\n",
    "\n",
    "# getting dictionary of items\n",
    "c = Counter(test)\n",
    "\n",
    "# Getting taxi_count for relevant grid_num\n",
    "df_taxicount = pd.DataFrame({'grid_num': [float(x) for x in list(c.keys())], \n",
    "                             'taxi_count': [x[1] for x in list(c.items())]})\n",
    "\n",
    "# Get full list of grid_num as a dataframe:  grid_num | timestamp\n",
    "all_grids = grids[['grid_num']]\n",
    "all_grids['timestamp'] = ts\n",
    "\n",
    "\n",
    "# Merge all_grids and df_taxicount\n",
    "taxi_clean = pd.merge(all_grids, df_taxicount, how='left')\n",
    "taxi_clean['taxi_count'] = taxi_clean['taxi_count'].fillna(0) #fill missing taxi_count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging all the cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge all together\n",
    "merge_df = pd.merge(humid_clean, rain_clean)\n",
    "merge_df = pd.merge(merge_df, temp_clean)\n",
    "merge_df = pd.merge(merge_df, taxi_clean)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['timestamp'] = merge_df['timestamp'].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S'))\n",
    "merge_df['hour'] = merge_df['timestamp'].apply(lambda x: x.hour)\n",
    "merge_df['month'] = merge_df['timestamp'].apply(lambda x: x.month)\n",
    "merge_df['day'] = merge_df['timestamp'].apply(lambda x: x.weekday())\n",
    "merge_df['minute'] = merge_df['timestamp'].apply(lambda x: x.minute)\n",
    "\n",
    "merge_df['time_30'] = merge_df['timestamp'].apply(lambda x: x + timedelta(hours=0.5))\n",
    "merge_df['time_60'] = merge_df['timestamp'].apply(lambda x: x + timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get y_30 and y_60 targets\n",
    "# for each timestamp, get 30min later\n",
    "merge_df['y_30'] = np.nan\n",
    "merge_df['y_60'] = np.nan\n",
    "\n",
    "for i in tqdm(range(len(merge_df))): # for each row\n",
    "    ts = merge_df.iloc[i]['time_30']\n",
    "    gridnum = merge_df.iloc[i]['grid_num']\n",
    "    \n",
    "    merge_df.iloc[i, merge_df.columns.get_loc('y_30')] = merge_df[(merge_df['grid_num'] == gridnum) & \n",
    "                                                                       (merge_df['timestamp'] == ts)].reset_index()['taxi_count'][0]\n",
    "    \n",
    "\n",
    "for i in tqdm(range(len(merge_df))): # for each row\n",
    "    ts = merge_df.iloc[i]['time_60']\n",
    "    gridnum = merge_df.iloc[i]['grid_num']\n",
    "    \n",
    "    merge_df.iloc[i, merge_df.columns.get_loc('y_60')] = merge_df[(merge_df['grid_num'] == gridnum) & \n",
    "                                                                       (merge_df['timestamp'] == ts)].reset_index()['taxi_count'][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge_df is the final dataset, ready to be used for EDA/ training etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
